# 02456_Group_44_DDPM_Project

![generated_images_for_MNIST](./resources/gen_img.png)

This repository contains a project implemented by a group of university students at the Technical University of Denmark (DTU). The aim of the project is to re-implement the Denoising Diffusion Probabilistic Model (DDPM) in PyTorch and reproduce their results at least on MNIST and ideally on CIFAR-10. This paper is the one that kicked off the diffusion movement, it is a great way to learn what diffusion is all about and have hands-on experience. Link to paper: https://arxiv.org/abs/2006.11239.

# Repository Structure
```
├── docs                    # Documentation, report and poster
├── results                 # Logs, images, plots, generated by models during training 
├── resources               # Resources used for README.md
└── src                     # source code
    ├── classifier_free_guidance      # all classes needed to train and use classifier-free-guided model
    ├── classifier_guidance           # all classes needed to train and use classifier-guided model
    ├── dataset.py              # Dataloader, Dataset
    ├── diffusion_model.py      # Diffusion model adaptable to a custom architecture
    ├── generator.py            # Generate samples from a trained Diffusion model
    ├── logger.py               # Log training steps, save best model, plot
    ├── main.py                 # Main code (used for training)
    ├── metrics.py              # Inception and FID score calculation
    ├── objective.py            # Noise loss objective in PyTorch format
    ├── postprocessing.py       # Script to calculate Inception/FID score after training
    ├── schedule.py             # Noise schedules (linear, cosine) to generate variance
    ├── trainer.py              # Model training
    ├── unet.py                 # Base architectures and blocks (UNet, CNN, ResNet)
    ├── utils.py                # Extra functions (normalize, upsample, time embeddings, etc.)
    └── visualizer.py           # Plot generation, model outputs, etc.
├── requirements.txt        # Pip package dependencies
```
# Prerequisites
To be able to run the scripts in the repository, an environment with all the dependencies of `requirements.txt` is needed.

# Running the scripts
### Python
To train a DDPM, call `main.py` via your python interpreter. The following optional arguments are supported:
```bash
python src/main.py [ARGS]

List of Arguments:
  -h, --help            show this help message and exit
  --data_type {mnist,cifar10}
                        Dataset to use: 'mnist' or 'cifar10'
  --schedule {linear,cosine}
                        Schedule type: 'linear' or 'cosine'
  --attention {attention,noattention}
                        Attention type: 'attention' or 'noattention'
```

To generate images for a pretrained DDPM, call `generator.py` via your python interpreter. The following optional arguments are supported:
```bash
python src/generator.py [ARGS]

List of Arguments::
  -h, --help            show this help message and exit
  --mode {default,guided_classifier,guided_free}
                        Model mode: 'default' or 'guided_classifier' or 'guided_free'
  --data_type {mnist,cifar10}
                        Dataset to use: 'mnist' or 'cifar10'
  --schedule {linear,cosine}
                        Schedule type: 'linear' or 'cosine'
  --attention {attention,noattention}
                        Attention type: 'attention' or 'noattention'
  --model MODEL         Model path: relative path to the trained UNet to use for sampling
  --guided_class GUIDED_CLASS
                        Guided class: Class to use for guided sampling
```

To calculate the FID score for a pretrained model, call `postprocessing.py` via your python interpreter. The following optional arguments are supported:
```bash
python src/postprocessing.py [ARGS]

options:
  -h, --help            show this help message and exit
  --data_type {mnist,cifar10}
                        Dataset to use: 'mnist' or 'cifar10'
  --schedule {linear,cosine}
                        Schedule type: 'linear' or 'cosine'
  --attention {attention,noattention}
                        Attention type: 'attention' or 'noattention'
  --model MODEL         Model path: relative path to the trained UNet to use for sampling
  --num_samples NUM_SAMPLES
                        Number of samples: chhose number of samples to calculate FID-score on. Should be multiple of batch size.
  --batch_size BATCH_SIZE
                        Batch size: choose batch size for FID calculation
```

### Jupyter Notebook
Alternatively, you may replicate our results via the Jupyter Notebook at `src/main.ipynb`, which can be used to plot and sample from an already trained model.

# Report and Poster

The project report and the poster may be found [here](/docs/Report_DDPM_Group_44.pdf) and [here](/docs/Poster_DDPM_Group_44.pdf). The reulsts shown in poth are also uploaded to the results directory of this repository.

# Authors
This project has been created by:
- Zeljko Antunovic (s233025)
- Alex Belai (s233423)
- Lukas Samuel Czekalla (s233561)
- Nandor Takacs (s232458)
